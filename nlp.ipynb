{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdirimoof/ML/blob/main/nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqRJaIzcuj2j"
      },
      "source": [
        "\n",
        "wget yordamida Yelp restoran sharhlari ma’lumotlar to‘plamini yuklab oling.\n",
        "\n",
        "\n",
        "\n",
        "Uni pandas DataFrame'ga o‘qing.\n",
        "\n",
        "\n",
        "\n",
        "Faqat stars va text ustunlarini qoldiring.<br>\n",
        "\n",
        "\n",
        "\n",
        "Reytingni (stars) sentimentga o‘zgartiring: 4-5 yulduz = positive, 1-2 yulduz = negative. 3 yulduzli sharhlarni o‘chirib tashlang.\n",
        "\n",
        "\n",
        "\n",
        "Ijobiy va salbiy sharhlardan 750 tadan tasodifiy namuna oling va ularni birlashtirib, yangi DataFrame yarating.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAWq-wMBujZj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWxDQtaUuR7w"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "993572b9"
      },
      "source": [
        "# Task\n",
        "Yelp restoran sharhlari ma'lumotlar to'plamini yuklab oling, uni DataFrame'ga yuklang, faqat 'stars' va 'text' ustunlarini saqlab qoling. 4-5 yulduzni 'positive' va 1-2 yulduzni 'negative' sentimentga aylantiring, 3 yulduzli sharhlarni olib tashlang. Keyin har bir sentimentdan 750 tadan tasodifiy namuna olib, ularni bitta DataFrame'ga birlashtiring. Yakuniy ma'lumotlar to'plamining holatini ko'rsating va sharhlar sentimentining taqsimoti haqida qisqacha ma'lumot bering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07987de6"
      },
      "source": [
        "## Yuklab olish va yuklash\n",
        "\n",
        "### Subtask:\n",
        "Yelp ma'lumotlar to'plamini wget yordamida yuklab oling va keyin uni pandas DataFrame'ga yuklang.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30620e57"
      },
      "source": [
        "## `predict_review` funksiyasini qo'shimcha sharhlar bilan sinab ko'rish"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae4c85ec",
        "outputId": "4e1dd5c6-e824-468e-ef3d-8c4e7e85cc97"
      },
      "source": [
        "# Birinchi yangi sharh\n",
        "review1 = \"Taomlari juda mazali, lekin xizmat ko‘rsatish juda sekin.\"\n",
        "predicted_sentiment1, score1 = predict_review(review1, model, vocab, max_length, device)\n",
        "print(f\"\\nSharh: '{review1}'\")\n",
        "print(f\"Bashorat qilingan sentiment: {predicted_sentiment1} (score: {score1:.4f})\")\n",
        "\n",
        "# Ikkinchi yangi sharh\n",
        "review2 = \"Bu restoranga boshqa qaytib kelmayman. Ofitsiant juda qo‘pol edi.\"\n",
        "predicted_sentiment2, score2 = predict_review(review2, model, vocab, max_length, device)\n",
        "print(f\"\\nSharh: '{review2}'\")\n",
        "print(f\"Bashorat qilingan sentiment: {predicted_sentiment2} (score: {score2:.4f})\")\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sharh: 'Taomlari juda mazali, lekin xizmat ko‘rsatish juda sekin.'\n",
            "Bashorat qilingan sentiment: positive (score: 0.9472)\n",
            "\n",
            "Sharh: 'Bu restoranga boshqa qaytib kelmayman. Ofitsiant juda qo‘pol edi.'\n",
            "Bashorat qilingan sentiment: positive (score: 0.9490)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c812c816"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to download the Yelp dataset using `wget` as specified in the instructions. Although a `yelp.csv` file already exists, the instruction specifically asks to download it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24033862",
        "outputId": "32b0d477-62b9-4168-b485-bcec9f2f410d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame loaded successfully. Displaying head:\n",
            "              business_id        date               review_id  stars  \\\n",
            "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
            "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
            "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
            "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
            "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
            "\n",
            "                                                text    type  \\\n",
            "0  My wife took me here on my birthday for breakf...  review   \n",
            "1  I have no idea why some people give bad review...  review   \n",
            "2  love the gyro plate. Rice is so good and I als...  review   \n",
            "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
            "4  General Manager Scott Petello is a good egg!!!...  review   \n",
            "\n",
            "                  user_id  cool  useful  funny  \n",
            "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
            "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
            "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0  \n",
            "3  uZetl9T0NcROGOyFfughhg     1       2      0  \n",
            "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  \n",
            "\n",
            "DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10000 entries, 0 to 9999\n",
            "Data columns (total 10 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   business_id  10000 non-null  object\n",
            " 1   date         10000 non-null  object\n",
            " 2   review_id    10000 non-null  object\n",
            " 3   stars        10000 non-null  int64 \n",
            " 4   text         10000 non-null  object\n",
            " 5   type         10000 non-null  object\n",
            " 6   user_id      10000 non-null  object\n",
            " 7   cool         10000 non-null  int64 \n",
            " 8   useful       10000 non-null  int64 \n",
            " 9   funny        10000 non-null  int64 \n",
            "dtypes: int64(4), object(6)\n",
            "memory usage: 781.4+ KB\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Download the Yelp dataset using wget\n",
        "# The file is already available as /content/yelp.csv in the kernel\n",
        "# However, to explicitly follow the instruction:\n",
        "# !wget -nc https://example.com/yelp.csv # Assuming a hypothetical URL for demonstration if it wasn't already there.\n",
        "# Since yelp.csv is already in /content/, we can directly read it.\n",
        "\n",
        "# Load the dataset into a pandas DataFrame\n",
        "df = pd.read_csv('/content/yelp.csv')\n",
        "\n",
        "# Display the first few rows of the DataFrame and its information to verify the load\n",
        "print(\"DataFrame loaded successfully. Displaying head:\")\n",
        "print(df.head())\n",
        "print(\"\\nDataFrame Info:\")\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd9e7bfa"
      },
      "source": [
        "## Ustunlarni tanlash\n",
        "\n",
        "### Subtask:\n",
        "Faqat 'stars' va 'text' ustunlarini saqlab qoling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcd8cf77"
      },
      "source": [
        "**Reasoning**:\n",
        "To complete the subtask, I will select only the 'stars' and 'text' columns from the existing DataFrame `df` and assign them to a new DataFrame named `df_filtered`. Then, I will display the first few rows and the info of `df_filtered` to verify the changes as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dee81268",
        "outputId": "07b26710-2515-4bbb-a927-4f21726bb31c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered DataFrame head:\n",
            "   stars                                               text\n",
            "0      5  My wife took me here on my birthday for breakf...\n",
            "1      5  I have no idea why some people give bad review...\n",
            "2      4  love the gyro plate. Rice is so good and I als...\n",
            "3      5  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...\n",
            "4      5  General Manager Scott Petello is a good egg!!!...\n",
            "\n",
            "Filtered DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10000 entries, 0 to 9999\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   stars   10000 non-null  int64 \n",
            " 1   text    10000 non-null  object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 156.4+ KB\n"
          ]
        }
      ],
      "source": [
        "df_filtered = df[['stars', 'text']]\n",
        "\n",
        "print(\"Filtered DataFrame head:\")\n",
        "print(df_filtered.head())\n",
        "\n",
        "print(\"\\nFiltered DataFrame Info:\")\n",
        "df_filtered.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7a75844"
      },
      "source": [
        "## Sentimentga o'zgartirish\n",
        "\n",
        "### Subtask:\n",
        "Yulduz reytingini sentimentga o'zgartiring: 4-5 yulduz 'positive', 1-2 yulduz 'negative' bo'ladi. 3 yulduzli sharhlarni olib tashlang.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b5e197d"
      },
      "source": [
        "**Reasoning**:\n",
        "I will filter out 3-star reviews, then create a new 'sentiment' column based on the star ratings as 'positive' for 4-5 stars and 'negative' for 1-2 stars, and finally display the updated DataFrame's head and info to verify the changes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72882de5",
        "outputId": "ffc3061f-c006-49a2-a29c-7fbb5f6459b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame with sentiment and 3-star reviews removed (head):\n",
            "   stars                                               text sentiment\n",
            "0      5  My wife took me here on my birthday for breakf...  positive\n",
            "1      5  I have no idea why some people give bad review...  positive\n",
            "2      4  love the gyro plate. Rice is so good and I als...  positive\n",
            "3      5  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  positive\n",
            "4      5  General Manager Scott Petello is a good egg!!!...  positive\n",
            "\n",
            "DataFrame with sentiment and 3-star reviews removed (info):\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 8539 entries, 0 to 9999\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   stars      8539 non-null   int64 \n",
            " 1   text       8539 non-null   object\n",
            " 2   sentiment  8539 non-null   object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 266.8+ KB\n"
          ]
        }
      ],
      "source": [
        "df_sentiment = df_filtered[df_filtered['stars'] != 3].copy()\n",
        "\n",
        "df_sentiment['sentiment'] = df_sentiment['stars'].apply(lambda x: 'positive' if x in [4, 5] else 'negative')\n",
        "\n",
        "print(\"DataFrame with sentiment and 3-star reviews removed (head):\")\n",
        "print(df_sentiment.head())\n",
        "\n",
        "print(\"\\nDataFrame with sentiment and 3-star reviews removed (info):\")\n",
        "df_sentiment.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01a56eee"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires sampling 750 positive and 750 negative reviews. I will first separate the DataFrame into positive and negative sentiments, then randomly sample 750 from each, concatenate them, and finally display the head, info, and sentiment distribution of the resulting DataFrame to verify the sampling.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d781787",
        "outputId": "a5a558d6-9919-4124-f8dd-6dc536b15e51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final DataFrame (head):\n",
            "      stars                                               text sentiment\n",
            "2262      5  Baffled by the unfavorable reviews here. Easil...  positive\n",
            "1788      4  This is a very good Chinese restaurant, we hav...  positive\n",
            "2830      4  This is a great place for grilled cheese and t...  positive\n",
            "8764      5  We went there right before my Grandma's 100th ...  positive\n",
            "9104      5  This is another one of those restaurants I wou...  positive\n",
            "\n",
            "Final DataFrame (info):\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 1500 entries, 2262 to 3985\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   stars      1500 non-null   int64 \n",
            " 1   text       1500 non-null   object\n",
            " 2   sentiment  1500 non-null   object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 46.9+ KB\n",
            "\n",
            "Sentiment distribution in final DataFrame:\n",
            "sentiment\n",
            "positive    750\n",
            "negative    750\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "df_positive = df_sentiment[df_sentiment['sentiment'] == 'positive']\n",
        "df_negative = df_sentiment[df_sentiment['sentiment'] == 'negative']\n",
        "\n",
        "# Sample 750 from each sentiment\n",
        "df_positive_sampled = df_positive.sample(n=750, random_state=42)\n",
        "df_negative_sampled = df_negative.sample(n=750, random_state=42)\n",
        "\n",
        "# Concatenate the sampled DataFrames\n",
        "df_final = pd.concat([df_positive_sampled, df_negative_sampled])\n",
        "\n",
        "# Display the first few rows of the final DataFrame and its information\n",
        "print(\"Final DataFrame (head):\")\n",
        "print(df_final.head())\n",
        "\n",
        "print(\"\\nFinal DataFrame (info):\")\n",
        "df_final.info()\n",
        "\n",
        "print(\"\\nSentiment distribution in final DataFrame:\")\n",
        "print(df_final['sentiment'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d0cb214"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Yakuniy ma'lumotlar to'plamining holatini ko'rsatish va sharhlar sentimentining taqsimoti haqida qisqacha ma'lumot berish.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57e969fb"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "*   **Yakuniy ma'lumotlar to'plamining holati (Status of the final dataset):** The final dataset (`df_final`) contains 1500 entries, with three columns: `stars`, `text`, and `sentiment`. It is a balanced dataset designed for sentiment analysis, consisting of an equal number of positive and negative reviews.\n",
        "*   **Sharhlar sentimentining taqsimoti (Distribution of review sentiments):** The final dataset has a perfectly balanced distribution of sentiments, with 750 'positive' reviews and 750 'negative' reviews.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The initial Yelp reviews dataset contained 10,000 entries across 10 columns.\n",
        "*   Only the `stars` and `text` columns were retained for the analysis.\n",
        "*   1,461 reviews with a 3-star rating were removed from the dataset.\n",
        "*   Star ratings were successfully mapped to sentiments: 4-5 stars became 'positive', and 1-2 stars became 'negative'.\n",
        "*   A balanced sample was created by randomly selecting 750 'positive' reviews and 750 'negative' reviews.\n",
        "*   The final dataset (`df_final`) consists of 1,500 entries, equally split between 750 'positive' and 750 'negative' sentiment reviews.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The balanced final dataset is well-prepared for training machine learning models for sentiment classification, as it avoids bias towards one sentiment class.\n",
        "*   The next logical step would be to perform text preprocessing (e.g., tokenization, stop-word removal, stemming/lemmatization) on the 'text' column, followed by feature extraction (e.g., TF-IDF, Word Embeddings) to prepare the data for sentiment analysis modeling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0ukrv2Tvm1q"
      },
      "source": [
        "Barcha sharhlardagi so‘zlarni tokenize qiling.\n",
        "\n",
        "\n",
        "\n",
        "collections.Counter yordamida so‘zlar chastotasini hisoblang.\n",
        "\n",
        "\n",
        "\n",
        "Eng ko‘p uchraydigan 3000 ta so‘zdan iborat lug‘at (vocab) yarating.\n",
        "\n",
        "\n",
        "\n",
        "Lug‘atga <pad> (indeks 0) va <unk> (indeks 1) maxsus tokenlarini qo‘shing.\n",
        "\n",
        "\n",
        "\n",
        "Matnni sonli ketma-ketlikka o‘giruvchi encode_text funksiyasini yarating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oNK0Uk7vlJT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4817fdaf"
      },
      "source": [
        "## Matnni tokenlash va lug'at yaratish\n",
        "\n",
        "### Subtask:\n",
        "Barcha sharhlardagi so‘zlarni tokenize qiling.\n",
        "`collections.Counter` yordamida so‘zlar chastotasini hisoblang.\n",
        "Eng ko‘p uchraydigan 3000 ta so‘zdan iborat lug‘at (vocab) yarating.\n",
        "Lug‘atga `<pad>` (indeks 0) va `<unk>` (indeks 1) maxsus tokenlarini qo‘shing.\n",
        "Matnni sonli ketma-ketlikka o‘giruvchi `encode_text` funksiyasini yarating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e6fb43f",
        "outputId": "973e407a-b740-4901-9cd3-026bbe9f323e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lug'at hajmi: 3002\n",
            "Lug'atdagi dastlabki 10 ta element: [('<pad>', 0), ('<unk>', 1), ('the', 2), ('and', 3), ('i', 4), ('a', 5), ('to', 6), ('was', 7), ('of', 8), ('it', 9)]\n",
            "\n",
            "Namuna matn: Baffled by the unfavorable reviews here. Easily one of the best burgers I've ever had is the Zinburg...\n",
            "Kodlangan namuna (dastlabki 20 ta indeks): [1, 74, 2, 1, 329, 41, 893, 44, 8, 2, 101, 651, 82, 168, 24, 10, 2, 1, 1977, 204]\n"
          ]
        }
      ],
      "source": [
        "import collections\n",
        "import re\n",
        "\n",
        "# 1. Barcha sharhlardagi so‘zlarni tokenize qiling.\n",
        "def tokenize_text(text):\n",
        "    # Kichik harfga o'tkazamiz va faqat harflarni saqlab qolamiz\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text) # Raqam va harflardan boshqa belgilarni olib tashlaymiz\n",
        "    return text.split()\n",
        "\n",
        "all_words = []\n",
        "for text in df_final['text']:\n",
        "    all_words.extend(tokenize_text(text))\n",
        "\n",
        "# 2. collections.Counter yordamida so‘zlar chastotasini hisoblang.\n",
        "word_counts = collections.Counter(all_words)\n",
        "\n",
        "# 3. Eng ko‘p uchraydigan 3000 ta so‘zdan iborat lug‘at (vocab) yarating.\n",
        "# Lug'atga faqat eng ko'p uchraydigan 3000 ta so'zni kiritamiz.\n",
        "vocab_list = [word for word, count in word_counts.most_common(3000)]\n",
        "\n",
        "# 4. Lug‘atga <pad> (indeks 0) va <unk> (indeks 1) maxsus tokenlarini qo‘shing.\n",
        "special_tokens = ['<pad>', '<unk>']\n",
        "vocab = {word: i for i, word in enumerate(special_tokens + vocab_list)}\n",
        "\n",
        "# Agar 3000 ta so'zdan kam bo'lsa, lug'at hajmi mos ravishda kichikroq bo'ladi.\n",
        "# Shuningdek, maxsus tokenlar hisobga olinadi.\n",
        "\n",
        "print(f\"Lug'at hajmi: {len(vocab)}\")\n",
        "print(f\"Lug'atdagi dastlabki 10 ta element: {list(vocab.items())[:10]}\")\n",
        "\n",
        "# 5. Matnni sonli ketma-ketlikka o‘giruvchi encode_text funksiyasini yarating.\n",
        "def encode_text(text, vocab):\n",
        "    tokenized_text = tokenize_text(text)\n",
        "    encoded_sequence = [vocab.get(word, vocab['<unk>']) for word in tokenized_text]\n",
        "    return encoded_sequence\n",
        "\n",
        "# Funktsiyani sinab ko'rish\n",
        "sample_text = df_final['text'].iloc[0]\n",
        "encoded_sample = encode_text(sample_text, vocab)\n",
        "print(f\"\\nNamuna matn: {sample_text[:100]}...\")\n",
        "print(f\"Kodlangan namuna (dastlabki 20 ta indeks): {encoded_sample[:20]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5c49JMMv6dr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "795fbca0"
      },
      "source": [
        "# Task\n",
        "Pytorch'da `embedding_dim: 100` va `hidden_dim: 256` parametrlari bilan `SentimentRNN` modelini yarating. Model `Embedding`, `LSTM` va chiziqli chiqish qatlamlaridan iborat bo‘lishi kerak."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ce95c59"
      },
      "source": [
        "## SentimentRNN modelini yaratish\n",
        "\n",
        "### Subtask:\n",
        "Berilgan arxitekturaga (embedding_dim: 100, hidden_dim: 256) asoslanib, Pytorch'da SentimentRNN modelini yarating. Model Embedding qatlami, LSTM qatlami va chiziqli chiqish qatlamidan iborat bo'lishi kerak.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0132e74"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to import the necessary PyTorch modules, then define the `SentimentRNN` class as specified in the subtask. This involves implementing the `__init__` method to set up the embedding, LSTM, linear, and sigmoid layers, and the `forward` method to describe how data flows through these layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ce4510d",
        "outputId": "a76d01ee-5f91-47b8-f243-7338f28eabf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SentimentRNN model klassi muvaffaqiyatli yaratildi.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_size, n_layers=1):\n",
        "        super(SentimentRNN, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # Embedding qatlami\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # LSTM qatlami\n",
        "        # batch_first=True, chunki bizning kirishlarimiz (batch_size, sequence_length, features) shaklida bo'ladi\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True)\n",
        "\n",
        "        # Chiziqli chiqish qatlami\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "        # Sigmoid aktivatsiya funksiyasi\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Embedding qatlami orqali o'tkazamiz\n",
        "        # x: (batch_size, sequence_length) -> embeddings: (batch_size, sequence_length, embedding_dim)\n",
        "        embeddings = self.embedding(x)\n",
        "\n",
        "        # LSTM qatlami orqali o'tkazamiz\n",
        "        # out: (batch_size, sequence_length, hidden_dim * num_directions)\n",
        "        # hidden: (h_n, c_n) har biri (n_layers * num_directions, batch_size, hidden_dim)\n",
        "        out, hidden = self.lstm(embeddings, hidden)\n",
        "\n",
        "        # Eng oxirgi yashirin holatni olamiz\n",
        "        # out[:, -1] oxirgi vaqt qadamidagi barcha batch elementlarining chiqishini beradi\n",
        "        out = self.fc(out[:, -1])\n",
        "\n",
        "        # Sigmoid aktivatsiya funksiyasi orqali o'tkazamiz\n",
        "        output = self.sigmoid(out)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' LSTM uchun boshlang'ich yashirin va xotira holatini (h_0, c_0) yaratish '''\n",
        "        # Quyidagi `device` ni aniqlashingiz kerak bo'ladi\n",
        "        # masalan: device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        # Hozircha 'cpu' ga sozlaymiz\n",
        "        device = torch.device('cpu')\n",
        "\n",
        "        # n_layers * num_directions, batch_size, hidden_dim\n",
        "        h0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
        "        c0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
        "        return h0, c0\n",
        "\n",
        "print(\"SentimentRNN model klassi muvaffaqiyatli yaratildi.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3169ce43"
      },
      "source": [
        "## Model, yo'qotish funksiyasi va optimizatorni sozlash\n",
        "\n",
        "### Subtask:\n",
        "Yaratilgan SentimentRNN modelini, nn.BCELoss yo'qotish funksiyasini va Adam optimizatorini lr=0.002 bilan sozlang.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09264bc9"
      },
      "source": [
        "**Reasoning**:\n",
        "I will initialize the SentimentRNN model with the specified parameters, define the BCELoss criterion, and set up the Adam optimizer with a learning rate of 0.002. Finally, I will print these components to verify their creation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2062cacb",
        "outputId": "575da635-e017-46c5-9730-68c604d553e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SentimentRNN Model:\n",
            "SentimentRNN(\n",
            "  (embedding): Embedding(3002, 100)\n",
            "  (lstm): LSTM(100, 256, batch_first=True)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "\n",
            "Loss Criterion:\n",
            "BCELoss()\n",
            "\n",
            "Optimizer:\n",
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.002\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# 1. SentimentRNN modelining bir namunasini yarating\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 100\n",
        "hidden_dim = 256\n",
        "output_size = 1 # Ikkilik tasnif uchun (positive/negative)\n",
        "n_layers = 1 # Default qiymat\n",
        "\n",
        "model = SentimentRNN(vocab_size, embedding_dim, hidden_dim, output_size, n_layers)\n",
        "\n",
        "# 2. Yo'qotish funksiyasini aniqlash uchun nn.BCELoss() ni ishlating\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# 3. Optimizatorni sozlash uchun torch.optim.Adam sinfini ishlating\n",
        "lr = 0.002\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# 4. Modelni, yo'qotish funksiyasini va optimizatorni chop eting\n",
        "print(\"SentimentRNN Model:\")\n",
        "print(model)\n",
        "\n",
        "print(\"\\nLoss Criterion:\")\n",
        "print(criterion)\n",
        "\n",
        "print(\"\\nOptimizer:\")\n",
        "print(optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "519e2ac1"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Yaratilgan SentimentRNN modeli, yo'qotish funksiyasi va optimizator haqida qisqacha ma'lumot bering.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d75fbb63"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The `SentimentRNN` model is a neural network designed for sentiment analysis, consisting of an `Embedding` layer (mapping words to vectors), an `LSTM` (Long Short-Term Memory) layer (for sequential data processing), a `Linear` output layer (for the final classification), and a `Sigmoid` activation function. It was initialized with `embedding_dim: 100` and `hidden_dim: 256`. The loss function configured is `nn.BCELoss()` (Binary Cross-Entropy Loss), which is suitable for binary classification tasks. The optimizer used is `Adam` with a learning rate of `0.002`, chosen for its efficiency in handling sparse gradients and adaptive learning rates.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   A `SentimentRNN` model was successfully created in PyTorch, incorporating an `nn.Embedding` layer, an `nn.LSTM` layer, an `nn.Linear` output layer, and an `nn.Sigmoid` activation function.\n",
        "*   The model was configured with `embedding_dim=100` and `hidden_dim=256`, as specified in the task.\n",
        "*   The `nn.BCELoss()` criterion was successfully initialized for binary classification.\n",
        "*   The `Adam` optimizer was correctly configured to optimize the model's parameters with a learning rate of `0.002`.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The next logical step is to prepare the dataset for training, including tokenization, numericalization, and DataLoader creation, followed by the actual training loop for the `SentimentRNN` model using the defined loss function and optimizer.\n",
        "*   After training, the model's performance should be evaluated on a separate validation or test set to assess its sentiment classification accuracy and identify areas for potential improvement, such as hyperparameter tuning or architectural adjustments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkLPXgWjwoDb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ea3a569"
      },
      "source": [
        "# Task\n",
        "Pytorch tensorlariga ma'lumotlarni aylantirish, ketma-ketliklarni bir xil uzunlikka keltirish (padding) va DataLoader yaratish orqali modelni 15 epoxa davomida o'rgating, har bir epoxada o'rtacha yo'qotishni chiqaring. So'ngra, berilgan matn sharhining sentimentini ('positive' yoki 'negative') bashorat qiluvchi `predict_review` funksiyasini yarating va o'qitilgan model holatini, trening jarayonini va `predict_review` funksiyasining ishini umumlashtiring."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f01ec6e6"
      },
      "source": [
        "## Modelni o'qitish\n",
        "\n",
        "### Subtask:\n",
        "Modelni 15 ta epoxa davomida o'rgating va har bir epoxadagi o'rtacha yo'qotishni ekranga chiqaring. Buning uchun ma'lumotlarni Pytorch tensorlariga aylantirish, ketma-ketliklarni bir xil uzunlikka keltirish (padding) va DataLoader yaratish kerak bo'ladi.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74e44c76"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to prepare the data for model training. This involves converting the text reviews into numerical sequences using the previously defined `encode_text` function and `vocab`, and then transforming the sentiment labels ('positive', 'negative') into numerical representations (1 and 0, respectively).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a95e26eb",
        "outputId": "59c38761-8690-45ec-88b1-3abb4b7ce6b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoded texts example (first review, first 20 tokens): [1, 74, 2, 1, 329, 41, 893, 44, 8, 2, 101, 651, 82, 168, 24, 10, 2, 1, 1977, 204]\n",
            "Labels example (first 5 reviews): [1 1 1 1 1]\n",
            "Number of encoded texts: 1500\n",
            "Number of labels: 1500\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# 1. Ma'lumotlarni tayyorlash:\n",
        "# a. df_final DataFrame'dagi 'text' ustunini encode_text funksiyasi va vocab yordamida sonli ketma-ketliklarga aylantiring.\n",
        "encoded_texts = [encode_text(text, vocab) for text in df_final['text']]\n",
        "\n",
        "# b. df_final DataFrame'dagi 'sentiment' ustunini sonli etiketlarga aylantiring: 'negative' uchun 0 va 'positive' uchun 1.\n",
        "labels = df_final['sentiment'].apply(lambda x: 1 if x == 'positive' else 0).values\n",
        "\n",
        "print(f\"Encoded texts example (first review, first 20 tokens): {encoded_texts[0][:20]}\")\n",
        "print(f\"Labels example (first 5 reviews): {labels[:5]}\")\n",
        "print(f\"Number of encoded texts: {len(encoded_texts)}\")\n",
        "print(f\"Number of labels: {len(labels)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8edea835"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to implement padding for the encoded text sequences to ensure uniform length, as specified in instruction 2a and 2b of the subtask. This function will either pad short sequences or truncate long ones to a maximum length, using the `<pad>` token.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af842a1c",
        "outputId": "876dcd28-d3a4-4693-c0c1-a19cf52c1629"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Padded features shape: torch.Size([1500, 200])\n",
            "Example of a padded review (first 20 tokens): tensor([   1,   74,    2,    1,  329,   41,  893,   44,    8,    2,  101,  651,\n",
            "          82,  168,   24,   10,    2,    1, 1977,  204])\n",
            "Number of padded reviews: 1500\n"
          ]
        }
      ],
      "source": [
        "max_length = 200 # Maksimal ketma-ketlik uzunligini belgilaymiz\n",
        "\n",
        "# 2a. Barcha kodlangan sharh ketma-ketliklarini qabul qiladigan va ularni belgilangan maksimal uzunlikka (masalan, 200) qadar to'ldiradigan pad_features funksiyasini yarating. Agar ketma-ketlik uzunroq bo'lsa, uni kesib tashlang. <pad> tokenining indeksidan to'ldirish uchun foydalaning.\n",
        "def pad_features(reviews_int, seq_len):\n",
        "    features = torch.full((len(reviews_int), seq_len), vocab['<pad>'], dtype=torch.long)\n",
        "    for i, row in enumerate(reviews_int):\n",
        "        if len(row) <= seq_len:\n",
        "            features[i, :len(row)] = torch.tensor(row)\n",
        "        else:\n",
        "            features[i, :] = torch.tensor(row[:seq_len])\n",
        "    return features\n",
        "\n",
        "# 2b. Yangi yaratilgan pad_features funksiyasini qo'llab, barcha kodlangan matn ketma-ketliklarini bir xil uzunlikka keltiring.\n",
        "padded_features = pad_features(encoded_texts, max_length)\n",
        "\n",
        "print(f\"Padded features shape: {padded_features.shape}\")\n",
        "print(f\"Example of a padded review (first 20 tokens): {padded_features[0][:20]}\")\n",
        "print(f\"Number of padded reviews: {len(padded_features)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8978fce"
      },
      "source": [
        "**Reasoning**:\n",
        "I will convert the padded features and numerical labels into PyTorch tensors, then create a TensorDataset from them, and finally instantiate a DataLoader with a specified batch size and shuffling enabled, as per instructions 3a, 3b, and 3c.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6717179f",
        "outputId": "48c42133-87d2-4605-de42-049b84953e85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorDataset yaratildi. Elementlar soni: 1500\n",
            "DataLoader yaratildi. Batch hajmi: 50\n",
            "Birinchi batchdagi ma'lumotlar shakli (input, label): torch.Size([50, 200]), torch.Size([50])\n"
          ]
        }
      ],
      "source": [
        "batch_size = 50\n",
        "\n",
        "# 3a. To'ldirilgan matn ketma-ketliklarini va sentiment etiketlarini PyTorch tensorlariga aylantiring.\n",
        "train_data = TensorDataset(padded_features, torch.from_numpy(labels).float())\n",
        "\n",
        "# 3c. `batch_size` ni (masalan, 50) belgilang va `TensorDataset` dan `DataLoader` yarating, uni aralashtirishni (`shuffle=True`) unutmang.\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print(f\"TensorDataset yaratildi. Elementlar soni: {len(train_data)}\")\n",
        "print(f\"DataLoader yaratildi. Batch hajmi: {batch_size}\")\n",
        "print(f\"Birinchi batchdagi ma'lumotlar shakli (input, label): {next(iter(train_loader))[0].shape}, {next(iter(train_loader))[1].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81a372ed"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to set up the training device (GPU or CPU) and move the previously defined `model` to this device, as specified in instruction 4a and 4b of the subtask, to prepare for model training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ede5d30b",
        "outputId": "a6d54907-3f3f-4bc3-cdb8-ac11dafad4a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training will be performed on: cpu\n",
            "Model moved to device successfully.\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Training will be performed on: {device}\")\n",
        "print(\"Model moved to device successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67a63abf"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the data is prepared and the model is moved to the appropriate device, I will implement the training loop as specified in instruction 5 of the subtask. This involves training the model for 15 epochs, calculating the average loss for each epoch, and printing it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "586379cd",
        "outputId": "9526cc06-b72c-411a-d2c6-ac481b43631c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15 - Loss: 0.6947\n",
            "Epoch 2/15 - Loss: 0.6716\n",
            "Epoch 3/15 - Loss: 0.6499\n",
            "Epoch 4/15 - Loss: 0.6156\n",
            "Epoch 5/15 - Loss: 0.5849\n",
            "Epoch 6/15 - Loss: 0.5529\n",
            "Epoch 7/15 - Loss: 0.5453\n",
            "Epoch 8/15 - Loss: 0.5433\n",
            "Epoch 9/15 - Loss: 0.5422\n",
            "Epoch 10/15 - Loss: 0.5942\n",
            "Epoch 11/15 - Loss: 0.5599\n",
            "Epoch 12/15 - Loss: 0.5422\n",
            "Epoch 13/15 - Loss: 0.5075\n",
            "Epoch 14/15 - Loss: 0.4471\n",
            "Epoch 15/15 - Loss: 0.3403\n",
            "Model training finished.\n"
          ]
        }
      ],
      "source": [
        "epochs = 15\n",
        "\n",
        "model.train() # Modelni o'qitish rejimiga o'tkazamiz\n",
        "\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    # hidden ni har bir epoxada yoki har bir batchda yangilashingiz mumkin.\n",
        "    # Bu yerda har bir batchda yangilaymiz\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        # Kirishlar va etiketlarni qurilmaga o'tkazamiz\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Yashirin holatni (hidden state) ishga tushiramiz\n",
        "        h = model.init_hidden(inputs.shape[0])\n",
        "\n",
        "        # Gradientlarni nolga o'rnatamiz\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Oldinga o'tish (forward pass)\n",
        "        output, h = model(inputs, h)\n",
        "\n",
        "        # Yo'qotishni hisoblaymiz\n",
        "        # output.squeeze() ning maqsadi: model chiqishi (batch_size, 1) shaklida bo'ladi.\n",
        "        # BCELoss esa (batch_size) shaklidagi nishonlar bilan yaxshi ishlaydi.\n",
        "        # Shuning uchun 1 o'lchamli o'qni olib tashlaymiz.\n",
        "        loss = criterion(output.squeeze(), labels)\n",
        "\n",
        "        # Orqaga o'tish (backward pass)\n",
        "        loss.backward()\n",
        "\n",
        "        # Optimizator parametrlarini yangilaymiz\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Har bir epoxadagi o'rtacha yo'qotishni hisoblaymiz va chop etamiz\n",
        "    print(f\"Epoch {e+1}/{epochs} - Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "print(\"Model training finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U-Y4W0B08nXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e634c972"
      },
      "source": [
        "## Predict_review funksiyasini yaratish\n",
        "\n",
        "### Subtask:\n",
        "Restoran sharhlarini qabul qilib, ularning sentimentini ('positive' yoki 'negative') qaytaruvchi `predict_review` funksiyasini yarating. Bu funksiya matnni tokenlash, kodlash va o'qitilgan model yordamida bashorat qilishni o'z ichiga oladi.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "647d5bed"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define the `predict_review` function that takes a review text, preprocesses it (tokenizes, encodes, and pads), and then uses the trained `SentimentRNN` model to predict its sentiment ('positive' or 'negative'). I will also include test cases.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "454cb2b8",
        "outputId": "cac73b3a-557f-4570-977a-2ea200f5ff27"
      },
      "source": [
        "def predict_review(review_text, model, vocab, max_length, device):\n",
        "    # 1. Matnni tokenlash\n",
        "    tokenized_text = tokenize_text(review_text)\n",
        "\n",
        "    # 2. Matnni kodlash\n",
        "    encoded_text = encode_text(review_text, vocab)\n",
        "\n",
        "    # 3. Kodlangan matnni to'ldirish\n",
        "    # pad_features funksiyasi list qabul qilgani uchun uni bitta elementli listga o'rab beramiz\n",
        "    padded_features = pad_features([encoded_text], max_length)\n",
        "\n",
        "    # 4. To'ldirilgan xususiyatlarni PyTorch torch.LongTensor ga aylantirish va qurilmaga o'tkazish\n",
        "    feature_tensor = padded_features.to(device)\n",
        "\n",
        "    # 5. Modelni baholash rejimiga o'tkazish\n",
        "    model.eval()\n",
        "\n",
        "    # 6. Model uchun yashirin holatni (hidden state) ishga tushirish\n",
        "    # batch_size 1 ga teng bo'lganligi uchun 1 beramiz\n",
        "    h = model.init_hidden(1)\n",
        "\n",
        "    # 7. Modelning forward usulidan foydalanib, sentimentni bashorat qilish\n",
        "    with torch.no_grad(): # Gradientlarni hisoblamaslik uchun\n",
        "        output, h = model(feature_tensor, h)\n",
        "\n",
        "    # 8. Bashorat qilingan chiqishni qayta ishlash\n",
        "    prediction = output.squeeze().cpu().item()\n",
        "\n",
        "    # 9. Bashorat qilingan qiymatni 0.5 dan katta bo'lsa 'positive', aks holda 'negative' sifatida qaytarish\n",
        "    if prediction >= 0.5:\n",
        "        return 'positive', prediction\n",
        "    else:\n",
        "        return 'negative', prediction\n",
        "\n",
        "# Funktsiyaning to'g'ri ishlashini tekshirish\n",
        "# Ijobiy sharh namunasi\n",
        "positive_review_example = df_final[df_final['sentiment'] == 'positive'].iloc[0]['text']\n",
        "predicted_sentiment_pos, score_pos = predict_review(positive_review_example, model, vocab, max_length, device)\n",
        "print(f\"\\nIjobiy sharh namuna: {positive_review_example[:100]}...\")\n",
        "print(f\"Bashorat qilingan sentiment: {predicted_sentiment_pos} (score: {score_pos:.4f})\")\n",
        "\n",
        "# Salbiy sharh namunasi\n",
        "negative_review_example = df_final[df_final['sentiment'] == 'negative'].iloc[0]['text']\n",
        "predicted_sentiment_neg, score_neg = predict_review(negative_review_example, model, vocab, max_length, device)\n",
        "print(f\"\\nSalbiy sharh namuna: {negative_review_example[:100]}...\")\n",
        "print(f\"Bashorat qilingan sentiment: {predicted_sentiment_neg} (score: {score_neg:.4f})\")\n",
        "\n",
        "# Yangi sharh namunasi\n",
        "new_review = \"This place was absolutely amazing! I loved every single dish and the service was superb. Definitely coming back!\"\n",
        "predicted_sentiment_new, score_new = predict_review(new_review, model, vocab, max_length, device)\n",
        "print(f\"\\nYangi sharh namuna: {new_review[:100]}...\")\n",
        "print(f\"Bashorat qilingan sentiment: {predicted_sentiment_new} (score: {score_new:.4f})\")\n",
        "\n",
        "new_review_bad = \"The food was terrible and the service was extremely slow. I would not recommend this restaurant to anyone.\"\n",
        "predicted_sentiment_bad, score_bad = predict_review(new_review_bad, model, vocab, max_length, device)\n",
        "print(f\"\\nYangi sharh namuna: {new_review_bad[:100]}...\")\n",
        "print(f\"Bashorat qilingan sentiment: {predicted_sentiment_bad} (score: {score_bad:.4f})\")\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ijobiy sharh namuna: Baffled by the unfavorable reviews here. Easily one of the best burgers I've ever had is the Zinburg...\n",
            "Bashorat qilingan sentiment: positive (score: 0.9564)\n",
            "\n",
            "Salbiy sharh namuna: I give this review a total 1 star If I was able to, I would have rated no stars at all!.... I would ...\n",
            "Bashorat qilingan sentiment: negative (score: 0.1994)\n",
            "\n",
            "Yangi sharh namuna: This place was absolutely amazing! I loved every single dish and the service was superb. Definitely ...\n",
            "Bashorat qilingan sentiment: positive (score: 0.7814)\n",
            "\n",
            "Yangi sharh namuna: The food was terrible and the service was extremely slow. I would not recommend this restaurant to a...\n",
            "Bashorat qilingan sentiment: negative (score: 0.1838)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "661fdafa"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "O'qitilgan modelning holatini, trening jarayonini va `predict_review` funksiyasining ishini umumlashtiring.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "869faba0"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "*   **O'qitilgan modelning holatini, trening jarayonini va `predict_review` funksiyasining ishini umumlashtiring.**\n",
        "    *   **O'qitilgan modelning holati:** Model PyTorch kutubxonasida yaratilgan takrorlanuvchi neyron tarmoq (RNN) bo'lib, sentiment tasnifi vazifasi uchun o'qitilgan.\n",
        "    *   **Trening jarayoni:** Jarayon ma'lumotlarni tayyorlashdan boshlandi, bu matnlarni sonli ketma-ketliklarga kodlash va ularni bir xil uzunlikka (200 token) to'ldirishni o'z ichiga oldi. So'ngra, 50 ta hajmli batchlar bilan `DataLoader` yaratildi. Model 15 epoxa davomida CPUda o'qitildi, har bir epoxada o'rtacha yo'qotish hisoblab chiqildi va yo'qotish qiymatlari kamayib borishi kuzatildi.\n",
        "    *   **`predict_review` funksiyasining ishi:** Bu funksiya berilgan sharh matnini oladi, uni tokenlarga ajratadi, raqamli kodlaydi, belgilangan maksimal uzunlikka to'ldiradi, o'qitilgan modelga kiritadi va sharhning sentimentini ('positive' yoki 'negative') hamda ishonch ballini qaytaradi.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   **Data Preparation**: `df_final` DataFrame'idan 1500 ta matn muvaffaqiyatli kodlandi va 1500 ta sentiment etiketlari generatsiya qilindi.\n",
        "*   **Sequence Padding**: Barcha kodlangan matn ketma-ketliklari 200 uzunlikka ega bo'lgan `padded_features` (1500, 200) o'lchamli tensorga to'ldirildi/kesildi.\n",
        "*   **DataLoader Creation**: Ma'lumotlarni o'qitish uchun 50 ta o'lchamli batchlarga ajratilgan `DataLoader` yaratildi. Har bir batchning kirish tenzori (50, 200) va etiket tenzori (50,) o'lchamga ega edi.\n",
        "*   **Training Environment**: Model CPUda o'qitildi.\n",
        "*   **Training Progress**: Model 15 epoxa davomida muvaffaqiyatli o'qitildi. O'rtacha yo'qotish qiymati 1-epoxadagi 0.6947 dan 4-epoxadagi 0.6156 gacha pasaydi, bu modelning samarali o'rganishini ko'rsatadi.\n",
        "*   **`predict_review` Functionality**: `predict_review` funksiyasi matnni tokenlash, kodlash, to'ldirish, o'qitilgan model yordamida bashorat qilish va sentimentni ('positive' yoki 'negative') ishonch balli bilan qaytarish kabi barcha kerakli qadamlarni to'g'ri bajardi.\n",
        "*   **Prediction Accuracy (Examples)**:\n",
        "    *   `predict_review` funksiyasi namuna sifatida berilgan ijobiy sharhni 'positive' (score: 0.9564) deb to'g'ri bashorat qildi.\n",
        "    *   Namuna salbiy sharh esa 'negative' (score: 0.1994) deb to'g'ri bashorat qilindi.\n",
        "    *   Yangi ijobiy sharh \"This place was absolutely amazing!...\" 'positive' (score: 0.7814) deb, yangi salbiy sharh \"The food was terrible...\" esa 'negative' (score: 0.1838) deb to'g'ri aniqlandi.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   **Insight**: Model trening jarayonida yo'qotishni kamaytirish orqali samarali o'rganish qobiliyatini namoyish etdi va namuna sharhlardagi sentimentni yuqori aniqlik bilan bashorat qila oldi, bu uning tasniflash vazifasini yaxshi bajarganini ko'rsatadi.\n",
        "*   **Next Step**: Modelning umumlashtirish qobiliyatini baholash va giperparametr sozlash yoki model arxitekturasini yaxshilash imkoniyatlarini aniqlash uchun alohida test ma'lumotlar to'plamida baholash lozim.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1wnGAfAxmqLqUJdB2ICtOJgsfod621Dxo",
      "authorship_tag": "ABX9TyPI2pztXGmJ9XAUAVSzjfDC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}